{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MainCode.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMoh6+QkPs3486mEmIljav"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iLdk5Gn0FOPu"},"source":["# **Imported Some important Modules**"]},{"cell_type":"code","metadata":{"id":"skDj4UivtFpX"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LSqMTE8AMnpB"},"source":["Linked my google drive with my google colab."]},{"cell_type":"code","metadata":{"id":"HxIQpF_mtZCb","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602864123632,"user_tz":-360,"elapsed":33154,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"5b0bd2ff-dccf-4ce6-8831-294dab4bfb1b"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CxKAI8ekMuVY"},"source":["changed to the directory where the dataset is located"]},{"cell_type":"code","metadata":{"id":"N7xcE6hMtqQ2","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602864130058,"user_tz":-360,"elapsed":1923,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"32d9f358-e266-47a4-b2a1-54791ddc3cc2"},"source":["cd gdrive/My Drive/Colab Notebooks/Hate Speech Detection"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab Notebooks/Hate Speech Detection\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3H_ZIS-3M14Z"},"source":["Read the dataset using powerful pandas library."]},{"cell_type":"code","metadata":{"id":"MhDKO8zutssj"},"source":["dataset = pd.read_csv('train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mApz6dARwFcI","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1602864137678,"user_tz":-360,"elapsed":1271,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"f85395bf-128b-4518-d5a0-17acb0ed1baa"},"source":["dataset['speech']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       #DhoniKeepsTheGlove | WATCH: Sports Minister K...\n","1       @politico No. We should remember very clearly ...\n","2       @cricketworldcup Guess who would be the winner...\n","3       Corbyn is too politically intellectual for #Bo...\n","4       All the best to #TeamIndia for another swimmin...\n","                              ...                        \n","5847    @davidfrum @trueblueusa1 That's cute and all, ...\n","5848    a recession issa comin' #maga #magamyass #fuck...\n","5849    #DoctorsFightBack  Will 'The Mad n Irrational ...\n","5850    #ShiningIndia #educatedindia or more like RUND...\n","5851    Could this be our new Prime Minister?     #Ric...\n","Name: speech, Length: 5852, dtype: object"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"s17S_WejM9VP"},"source":["imported all the necessary library for text processing."]},{"cell_type":"code","metadata":{"id":"5O_WQwq3ty0q"},"source":["import nltk\n","import string\n","from nltk.corpus import stopwords # library for stopwords\n","from nltk.stem import WordNetLemmatizer #library for lemmatizing\n","from sklearn.feature_extraction.text import CountVectorizer # library for vectoraization\n","from sklearn.feature_extraction.text import TfidfTransformer # library for tfidf tranformer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oG0rgfqeHRrR"},"source":["Again, it is necessary to download \"Stopwords\" and \"wordnet\" packages from NLTK."]},{"cell_type":"code","metadata":{"id":"PbGPZ1BLuCsY","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1602864149436,"user_tz":-360,"elapsed":1747,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"020b2f36-e33b-4964-c724-1b0fdfa5937e"},"source":["nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"vBvvOwsswcI-","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602843081603,"user_tz":-360,"elapsed":1148,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"e25ca2f3-9eab-427f-a742-5bf722368164"},"source":["string.punctuation"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"vtQ4CrVHyjYG"},"source":["lemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1oeChPrSHwsm"},"source":["The following \"text_process\" function takes a string as input. Then it removes all the punctuation from that string using first line of the code. For example. input sentence = \"hey, how! are you.\". After executing first line, output = ['h','e','y',' ', 'h', 'o','w',' ', 'a', 'r', 'e', ' ', 'y', 'o', 'u']. Then the second line join this list of alphabets into words. second line output = [\"hey\", \"how\", \"are\", \"you\"]. The third line does two works simultaneously. First, it removes all the stop words from the sentence. Then, it groups all the similar words to their base words. For example, {\"go\", \"goes\", \"gone\"} will grouped into \"go\". "]},{"cell_type":"code","metadata":{"id":"YFqg80vtyqRZ"},"source":["def text_process(speech):\n","    no_punctuation = [c for c in speech if c not in string.punctuation] #remove all the punctuation\n","    no_punctuation = ''.join(no_punctuation)\n","    return [lemmatizer.lemmatize(word) for word in no_punctuation.split() if word.lower() not in set(stopwords.words('english'))] # removing all the stopwords + Lemmatizing"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hIa5yyad92Y"},"source":["Currently, we have all the speeches as lists of tokens and now we need to convert each of those messages into a **vector** the SciKit Learn's algorithm models can work with.\r\n","\r\n","For turning tokens into vectors, these following steps have been done:\r\n","\r\n","\r\n","1.   Count how many times does a word occur in each message (Known as term frequency **TF**)\r\n","\r\n","\r\n","2.   Weigh the counts, so that frequent tokens get lower weight (inverse document frequency **IDF**)\r\n","\r\n","3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\r\n","\r\n","These is also known as **Bag of words** model.\r\n","\r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"Cz7pt-LzlwUP"},"source":["Each vector will have as many dimensions as there are unique words in the Speech corpus. We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts. Output will be a sparse matrix.\r\n"]},{"cell_type":"code","metadata":{"id":"cGXz_S3n0Jz5"},"source":["bow_transformer = CountVectorizer(analyzer= text_process).fit(dataset['speech']) #vectorization using CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez0ZLSfYzWUr"},"source":["speeches_bow = bow_transformer.transform(dataset['speech'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ntq4rDhUl9j8"},"source":["TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\r\n","\r\n","One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\r\n","\r\n","TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\r\n","\r\n","TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\r\n","\r\n","IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\r\n","\r\n","IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\r\n","\r\n","See below for a simple example.\r\n","\r\n","Example:\r\n","\r\n","Consider a document containing 100 words wherein the word cat appears 3 times.\r\n","\r\n","The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\r\n"]},{"cell_type":"code","metadata":{"id":"RtJs7XHFztVe"},"source":["tfidftransformer = TfidfTransformer().fit(speeches_bow) #normalization using tf&idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"at_W58kTzyVU"},"source":["speeches_tfidf = tfidftransformer.transform(speeches_bow)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-0311rnz2he","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1602843814860,"user_tz":-360,"elapsed":981,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"7e2b4187-9362-45e1-d7a4-d8d22f2de731"},"source":["speeches_tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5852, 24387)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"vRSx-5qkNTSs"},"source":["Now our data is ready to fit into the machine learning models. We use Artificial Neural Network to do the classification."]},{"cell_type":"markdown","metadata":{"id":"K6zcGcAnnYjE"},"source":["Following Libraries are required to implement the Artificial Neural Network model."]},{"cell_type":"code","metadata":{"id":"1IJRAGviz5l1"},"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RezeXHaY0GOh"},"source":["model = Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4AMbeAh0UmD"},"source":["model.add(Dense(24387, input_shape = (24387,), activation = 'relu')) # Inputlayer + hidden layer activation function using \"RELU\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mFgsARs0X0r"},"source":["model.add(Dense(4, activation='softmax')) # Outputlayer + Step function using \"softmax\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY6pmDUS0bMV"},"source":["model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics = ['accuracy']) # optimizing using \"Adam\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEbXwaUg0fzw"},"source":["speeches_array = speeches_tfidf.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAgrrj640l4a"},"source":["y = pd.get_dummies(dataset['label']) #one hot encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sKTSb9J44WP","colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"status":"ok","timestamp":1602864262082,"user_tz":-360,"elapsed":1106,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"a5c732a3-7b9c-480b-ee8f-0d35dfb98633"},"source":["y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>HATE</th>\n","      <th>NONE</th>\n","      <th>OFFN</th>\n","      <th>PRFN</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5847</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5848</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5849</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5850</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5851</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5852 rows × 4 columns</p>\n","</div>"],"text/plain":["      HATE  NONE  OFFN  PRFN\n","0        0     1     0     0\n","1        1     0     0     0\n","2        0     1     0     0\n","3        0     1     0     0\n","4        0     1     0     0\n","...    ...   ...   ...   ...\n","5847     0     0     0     1\n","5848     0     1     0     0\n","5849     0     0     1     0\n","5850     0     0     1     0\n","5851     0     0     0     1\n","\n","[5852 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"ZSBy_MN40sfP"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wS-AebaW0wrC"},"source":["X_train, X_test, y_train, y_test = train_test_split(speeches_array, y, test_size=0.20, random_state=42) # train-test split, test size = 20%"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XA0f0Kr02cq","colab":{"base_uri":"https://localhost:8080/","height":592},"executionInfo":{"status":"ok","timestamp":1602864388965,"user_tz":-360,"elapsed":117528,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"14998250-c9c9-4768-ccfc-057877f24bf2"},"source":["model.fit(X_train,y_train,batch_size=100, epochs= 15) # FITTING THE MODEL INTO ARTIFICIAL NEURAL NETWORK"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n"," 2/47 [>.............................] - ETA: 6s - loss: 1.3427 - accuracy: 0.4400WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0633s vs `on_train_batch_end` time: 0.1220s). Check your callbacks.\n","47/47 [==============================] - 7s 153ms/step - loss: 1.0241 - accuracy: 0.6056\n","Epoch 2/15\n","47/47 [==============================] - 7s 154ms/step - loss: 0.3182 - accuracy: 0.9069\n","Epoch 3/15\n","47/47 [==============================] - 7s 155ms/step - loss: 0.0756 - accuracy: 0.9810\n","Epoch 4/15\n","47/47 [==============================] - 7s 157ms/step - loss: 0.0351 - accuracy: 0.9889\n","Epoch 5/15\n","47/47 [==============================] - 7s 159ms/step - loss: 0.0249 - accuracy: 0.9930\n","Epoch 6/15\n","47/47 [==============================] - 8s 160ms/step - loss: 0.0222 - accuracy: 0.9927\n","Epoch 7/15\n","47/47 [==============================] - 8s 161ms/step - loss: 0.0189 - accuracy: 0.9930\n","Epoch 8/15\n","47/47 [==============================] - 8s 162ms/step - loss: 0.0146 - accuracy: 0.9947\n","Epoch 9/15\n","47/47 [==============================] - 8s 165ms/step - loss: 0.0114 - accuracy: 0.9955\n","Epoch 10/15\n","47/47 [==============================] - 8s 166ms/step - loss: 0.0118 - accuracy: 0.9955\n","Epoch 11/15\n","47/47 [==============================] - 8s 168ms/step - loss: 0.0121 - accuracy: 0.9966\n","Epoch 12/15\n","47/47 [==============================] - 8s 168ms/step - loss: 0.0110 - accuracy: 0.9955\n","Epoch 13/15\n","47/47 [==============================] - 8s 166ms/step - loss: 0.0095 - accuracy: 0.9962\n","Epoch 14/15\n","47/47 [==============================] - 8s 164ms/step - loss: 0.0110 - accuracy: 0.9964\n","Epoch 15/15\n","47/47 [==============================] - 8s 163ms/step - loss: 0.0118 - accuracy: 0.9957\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f51025cc320>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"u7RJT_9c0-DK"},"source":["predictions = model.predict(X_test) # PREDICTIONS "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6E3hZI111X_R"},"source":["pred = np.argmax(predictions, axis = 1)\n","y_t = y_test.to_numpy()\n","y_class = np.argmax(y_t, axis = 1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIM0BG4t4WDH"},"source":["from sklearn.metrics import classification_report, confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_13Q3afi1j9R","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1602864404394,"user_tz":-360,"elapsed":1079,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"8761c21e-b3f5-4c05-8e79-8f482eeff04d"},"source":["print(classification_report(pred,y_class))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.19      0.28      0.23       147\n","           1       0.82      0.67      0.74       908\n","           2       0.15      0.33      0.21        42\n","           3       0.26      0.46      0.33        74\n","\n","    accuracy                           0.59      1171\n","   macro avg       0.36      0.43      0.38      1171\n","weighted avg       0.68      0.59      0.63      1171\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7G9knOQKLw9e"},"source":["All the same work again done for \"Test Dataset\"."]},{"cell_type":"code","metadata":{"id":"3-ep7Jme1mwz"},"source":["test_data = pd.read_csv('test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VB1pAqQd1wLj"},"source":["bow_transformer1 = CountVectorizer(analyzer= text_process).fit(test_data['speech'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ywQWUUUX10uF"},"source":["speeches_bow1 = bow_transformer.transform(test_data['speech'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PuKivBx6WBEX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIURRfU512_B"},"source":["tfidftransformer1 = TfidfTransformer().fit(speeches_bow1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YloEE5ZY15rt"},"source":["speeches_tfidf1 = tfidftransformer.transform(speeches_bow1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UKTWRzEq18h7"},"source":["speeches_tfidf1 = speeches_tfidf1.toarray()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2AT16e-1_GI"},"source":["predictions1 = model.predict(speeches_tfidf1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EG3K7U7a2Baq"},"source":["pred1 = np.argmax(predictions1, axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1O2aGJx62I5s"},"source":["x1 = pd.DataFrame(test_data['id'], columns=['id'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD4Q8CWX2PBX"},"source":["y1 = pd.DataFrame(pred,columns=['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mn79obwS2RPT"},"source":["new_dataframe = pd.concat([x1, y1], axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmYTXxrJ2UJB","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1602775957063,"user_tz":-360,"elapsed":1584,"user":{"displayName":"rony chowdhory","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh52SaUqAkhHQpCjpER6nSOr3-suBBRRZiv_g4q=s64","userId":"11130560283162345145"}},"outputId":"fa863353-42ff-437b-c593-fdce58f64f54"},"source":["new_list = list(new_dataframe['label'])\n","i = 0\n","for each in new_list:\n","  if each == 0:\n","    new_dataframe['label'][i] = 'HATE'\n","  elif each == 1:\n","    new_dataframe['label'][i] = 'NONE'\n","  elif each == 2:\n","    new_dataframe['label'][i] = 'OFFN'\n","  elif each == 3:\n","    new_dataframe['label'][i] = 'PRFN'\n","  i = i + 1\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ijSWwjN92W4T"},"source":["np.savetxt(r'predict.txt', new_dataframe.values, fmt='%s', delimiter='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c0HaqHx1MAYZ"},"source":[""]}]}